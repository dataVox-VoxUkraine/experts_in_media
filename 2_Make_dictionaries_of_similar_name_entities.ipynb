{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Filter name entities and join them by similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "from uk_stemmer import UkStemmer\n",
    "from nltk import stem\n",
    "import pymorphy2\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer_ru = stem.snowball.SnowballStemmer(\"russian\") \n",
    "stemmer_ukr = UkStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_uk_path = '../dicts/names_ukr.txt' \n",
    "names_ru_path = '../dicts/names_ru.txt' \n",
    "with open(names_uk_path) as f:\n",
    "    names_uk = [name.strip() for name in f.readlines()]\n",
    "    \n",
    "with open(names_ru_path) as f:\n",
    "    names_ru = [name.strip() for name in f.readlines()]\n",
    "names = list(set(names_uk+names_ru))\n",
    "names_string = '|'.join(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_string_pat = re.compile(names_string)\n",
    "two_cap_pat = re.compile(r'^[А-ЯЇҐЄІ]\\S* [А-ЯЇҐЄІ]\\S*$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['link', 'language', 'all_ent_str', 'name_and_kw', 'kw_and_ent',\n",
       "       'names_and_kw_str', 'names_str'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = '../data/entities_may.csv'\n",
    "entities = pd.read_csv(filepath, index_col=[0])\n",
    "entities.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get PERS entities from all entities and filter them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pers_ent_from_all(all_ents_str):\n",
    "    if pd.notna(all_ents_str):\n",
    "        pers_ents = re.findall(r'<§§>PERS<§§>(.*?)<§§>([\\d\\.]+)', all_ents_str)\n",
    "        if pers_ents:\n",
    "            return [(ent[0], float(ent[1])) for ent in pers_ents]\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_pers_ent_str(news_entities):\n",
    "    res = []\n",
    "    if pd.notna(news_entities):\n",
    "        s = 0\n",
    "        for sent in news_entities.split('++'):\n",
    "            for ent in sent.split(';;'):\n",
    "                if '::PERS::' in ent:\n",
    "                    res.append('§'.join([str(s), ent]))\n",
    "            s += 1\n",
    "        return '%%'.join(res)\n",
    "    return None\n",
    "\n",
    "\n",
    "def list_of_tuples_to_str(tuples):\n",
    "    if isinstance(tuples, list):\n",
    "        res=['§'.join((str(t[0]), t[1], t[2], str(t[3]))) for t in tuples]\n",
    "        return '±±'.join(res)\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_kw_name_tuples(kw_names_str):\n",
    "    if pd.notna(kw_names_str):\n",
    "        res = []\n",
    "        for ent in kw_names_str.split('@+@'):\n",
    "            parts = ent.split('<+>')\n",
    "            res.append((int(parts[0]), parts[1], parts[2], float(parts[3])))\n",
    "        return res\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_pers_ent_tuple(news_entities):\n",
    "    res = []\n",
    "    if pd.notna(news_entities):\n",
    "        s = 0\n",
    "        for sent in news_entities.split('++'):\n",
    "            for ent in sent.split(';;'):\n",
    "                if '::PERS::' in ent:\n",
    "                    r, n = ent.split('::PERS::')\n",
    "                    name, score = n.rsplit('::', maxsplit=1)\n",
    "                    if (len(name)>1) and (name.upper()!=name) and (name.lower()!=name):\n",
    "                        res.append((s, r, name, float(score)))\n",
    "            s += 1\n",
    "        if len(res)>0:\n",
    "            return res\n",
    "    return None\n",
    "\n",
    "\n",
    "def filter_by_score(pers_tuples, ind1=2, ind2=3, iteration=1, min_score=0.5):\n",
    "    if isinstance(pers_tuples, list):\n",
    "        ents = pers_tuples.copy()\n",
    "        for pt in pers_tuples:\n",
    "            two_capital = re.search(r'[А-ЯЇҐЄІ].* .*[А-ЯЇҐЄІ]', pt[ind1])\n",
    "            if iteration == 1:\n",
    "                isname = re.search(names_string, pt[ind1])\n",
    "                if (not isname) and (not two_capital) and pt[ind2] < min_score:\n",
    "                    ents.remove(pt)\n",
    "            elif iteration == 2:\n",
    "                if (not two_capital) and pt[ind2] < min_score:\n",
    "                    ents.remove(pt)\n",
    "        if len(ents)>0:\n",
    "            return ents\n",
    "    return None\n",
    "\n",
    "\n",
    "def tuples_to_list(pers_tuples, ind=2):\n",
    "    if isinstance(pers_tuples, list):\n",
    "        return [tp[ind] for tp in pers_tuples]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting ALL name entities from entities string and filter them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities['pers_tuples'] = entities.all_ent_str.apply(get_pers_ent_from_all)\n",
    "entities['filt_pers_tuples'] = entities['pers_tuples'].apply(filter_by_score, ind1=0, ind2=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities['filt_pers_tuples'] = entities['filt_pers_tuples'].apply(filter_by_score, ind1=0, ind2=1, iteration=2)\n",
    "entities['filt_pers_list'] = entities['filt_pers_tuples'].apply(tuples_to_list, ind=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate similarity ratios for stemmed names in news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_names(names_list, stemmer):\n",
    "    if isinstance(names_list, list): \n",
    "        res=[]\n",
    "        for name in names_list:\n",
    "            stemmed_name = []\n",
    "            for word in name.split():\n",
    "                st = stemmer(word)\n",
    "                if word[0].isupper():\n",
    "                    st = st.capitalize()\n",
    "                stemmed_name.append(st)\n",
    "            res.append(' '.join(stemmed_name))\n",
    "        return res\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities['stem_pers_list'] = entities.apply(lambda row: \\\n",
    "                                            stem_names(row.filt_pers_list, stemmer_ukr.stem_word) if row.language=='uk'\\\n",
    "                                            else stem_names(row.filt_pers_list, stemmer_ru.stem), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_ratio_from_list(per_list, ratio=fuzz.partial_ratio):\n",
    "    if isinstance(per_list, list):\n",
    "        res = []\n",
    "        for i in range(len(per_list)):\n",
    "            for j in range(i+1, len(per_list)):\n",
    "#                 res.append((i, j, fuzz.WRatio(per_list[i], per_list[j])))\n",
    "#                 res.append((i, j, fuzz.partial_ratio(per_list[i], per_list[j])))\n",
    "                res.append((i, j, ratio(per_list[i], per_list[j])))\n",
    "        if len(res)>0:\n",
    "            return res\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 33s, sys: 1.65 s, total: 1min 34s\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "entities['stem_partial_ratio'] = entities.stem_pers_list.apply(get_similarity_ratio_from_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 29s, sys: 4.48 s, total: 6min 33s\n",
      "Wall time: 6min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "entities['stem_wratio'] = entities.stem_pers_list.apply(lambda x: get_similarity_ratio_from_list(x, fuzz.WRatio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Form similarity dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_dicts(pers_list, wratios, partial_ratios, top_lim=90, bot_lim=70, mid_lim=80):\n",
    "    if isinstance(partial_ratios, list):\n",
    "        res = {}\n",
    "        for i in range(len(wratios)):\n",
    "            tp = wratios[i]\n",
    "            if tp[2] >= top_lim:\n",
    "                res[tp[0]] = res.get(tp[0], []) + [tp[1]]\n",
    "            elif tp[2] >= bot_lim:\n",
    "                str1 = pers_list[tp[0]]\n",
    "                str2 = pers_list[tp[1]]\n",
    "                if (two_cap_pat.match(str1) and two_cap_pat.match(str2)):\n",
    "                    uk_name1 = names_string_pat.match(str1)\n",
    "                    uk_name2 = names_string_pat.match(str2)\n",
    "                    if uk_name1 and uk_name2:\n",
    "                        if uk_name1 != uk_name2:\n",
    "                            if not res.get(tp[0]):\n",
    "                                res[tp[0]] = res.get(tp[0], [])\n",
    "                            continue\n",
    "                        else:\n",
    "                            if re.search(r'\\s\\w{2}', str1).group()!=re.search(r'\\s\\w{2}', str2).group():\n",
    "                                if not res.get(tp[0]):\n",
    "                                    res[tp[0]] = res.get(tp[0], [])\n",
    "                                continue\n",
    "                if (tp[2] >= mid_lim) or (partial_ratios[i][2] >= mid_lim):\n",
    "                    res[tp[0]] = res.get(tp[0], []) + [tp[1]]\n",
    "                else:\n",
    "                    res[tp[0]] = res.get(tp[0], [])\n",
    "            else:\n",
    "                res[tp[0]] = res.get(tp[0], [])       \n",
    "        return res\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities['similarity_dicts'] = entities.apply(lambda row: \\\n",
    "                                              get_similarity_dicts(row.filt_pers_list,\\\n",
    "                                                                    row.stem_wratio, \\\n",
    "                                                                    row.stem_partial_ratio), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_names_dicts_ratios(id):\n",
    "    print('names:', entities.loc[id].stem_pers_list)\n",
    "    print('ratios:', entities.loc[id].stem_wratio)\n",
    "    print('dicts:', entities.loc[id].similarity_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform dicts to lists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ratio from tutple by names indices\n",
    "def get_ratio_by_ind(i, j, tuples):\n",
    "    for tp in tuples:\n",
    "        if tp[0]==i and tp[1]==j:\n",
    "            return tp[2]\n",
    "\n",
    "# if current value      \n",
    "def dict_ratio_is_bigger(sim_dict, wratios, key, value, max_ratio):\n",
    "    for k, v in sim_dict.items():\n",
    "        if k not in sim_dict[key] and value in sim_dict[k]:\n",
    "            ratio = get_ratio_by_ind(k, value, wratios)\n",
    "            if ratio > max_ratio:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def value_ratio_is_bigger(value, v_sim, max_ratio, wratios):\n",
    "    for v in v_sim:\n",
    "        if get_ratio_by_ind(value, v, wratios) > max_ratio:\n",
    "            return True\n",
    "    return False\n",
    "        \n",
    "    \n",
    "def sim_dicts_to_lists(sim_dict, wratios):\n",
    "    if pd.notna(sim_dict):\n",
    "        res = [] \n",
    "        for key, value in sim_dict.items():\n",
    "            same = []\n",
    "            res_list = [item for sublist in res for item in sublist]\n",
    "            if key not in res_list:\n",
    "                if len(value)==0:\n",
    "                    res.append([key])\n",
    "                else:\n",
    "                    same.append(key)\n",
    "                    for v in value:\n",
    "                        if v not in res_list:\n",
    "#                             print(key, v)\n",
    "                            curr_ratio = get_ratio_by_ind(key, v, wratios)\n",
    "                            if not dict_ratio_is_bigger(sim_dict, wratios, key, v, curr_ratio):\n",
    "#                                 print('not bigger')\n",
    "                                v_sim = sim_dict.get(v, [])\n",
    "                                if len(v_sim) > 0:\n",
    "                                    if set(v_sim).issubset(set(value)):\n",
    "#                                         print('is subset')\n",
    "                                        same.append(v)\n",
    "#                                         print(same)\n",
    "                                    else:\n",
    "                                        if value_ratio_is_bigger(v, v_sim, curr_ratio, wratios):\n",
    "                                            continue\n",
    "                                        else:\n",
    "                                            same.append(v)\n",
    "                                else:\n",
    "                                    same.append(v)\n",
    "#                                     print(same)\n",
    "            if len(same)>0:\n",
    "                res.append(same)\n",
    "        if len(sim_dict) not in [item for sublist in res for item in sublist]:\n",
    "            res.append([len(sim_dict)])\n",
    "                            \n",
    "        return res    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.6 s, sys: 131 ms, total: 30.7 s\n",
      "Wall time: 31.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "entities['names_sets'] = entities.apply(lambda row: sim_dicts_to_lists(row.similarity_dicts, row.stem_wratio), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names_sets_to_lists(names_sets):\n",
    "    if isinstance(names_sets, list):\n",
    "        res=['+'.join(map(str, s)) for s in names_sets]\n",
    "        return ';'.join(res)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities['names_sets_str'] = entities['names_sets'].apply(names_sets_to_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_lists(entities, id):\n",
    "    right_str = '0+6;1+4;2;3;5;7'\n",
    "    entities.loc[id, 'names_sets_str'] = right_str\n",
    "    entities['names_sets'] = entities.names_sets_str.apply(split_name_sets_to_list)\n",
    "    entities['string_names_sets'] = entities.apply(lambda row: get_names_by_num(row.names_sets, row.filt_pers_list), axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names_by_num(indices, names):\n",
    "    if isinstance(indices, list):\n",
    "        res = []\n",
    "        for ind_set in indices:\n",
    "            name_set = []\n",
    "            for ind in ind_set:\n",
    "                name_set.append(names[ind])\n",
    "            res.append(name_set)\n",
    "        return res\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities['string_names_sets'] = entities.apply(lambda row: get_names_by_num(row.names_sets, row.filt_pers_list), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select one name that represens every list of similar names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_name_from_set(name_sets, stemmer):\n",
    "    if isinstance(name_sets, list):\n",
    "        res=[]\n",
    "        for name_set in name_sets:\n",
    "            chosen = max(name_set, key=len)\n",
    "            for name in name_set:\n",
    "                isname = re.match(names_string, name)\n",
    "                two_capital = re.search(r'[А-ЯЇҐЄІ].* .*[А-ЯЇҐЄІ]', name)\n",
    "                if isname and two_capital:\n",
    "                    chosen = name\n",
    "                    break\n",
    "            stemmed_name = []\n",
    "            for word in chosen.split():\n",
    "                st = stemmer(word)\n",
    "                if word[0].isupper():\n",
    "                    st = st.capitalize()\n",
    "                stemmed_name.append(st)\n",
    "            res.append(' '.join(stemmed_name))\n",
    "        return res\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities['one_name_per_set'] = entities.apply(lambda row: \\\n",
    "                                            get_one_name_from_set(row.string_names_sets, stemmer_ukr.stem_word) if row.language=='uk'\\\n",
    "                                            else get_one_name_from_set(row.string_names_sets, stemmer_ru.stem), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lists_to_str(name_sets):\n",
    "    if isinstance(name_sets, list):\n",
    "        res=[]\n",
    "        for ns in name_sets:\n",
    "            res.append('<+>'.join(ns))\n",
    "        return '<@>'.join(res)\n",
    "    return None\n",
    "    \n",
    "def list_to_str(name_sets):\n",
    "    if isinstance(name_sets, list):\n",
    "        return '<@>'.join(name_sets)\n",
    "    return None\n",
    "\n",
    "def ratio_to_str(ratio_list):\n",
    "    if isinstance(ratio_list, list):\n",
    "        res = []\n",
    "        for rt in ratio_list:\n",
    "            res.append('+'.join([str(rt[0]), str(rt[1]), str(rt[2])]))\n",
    "        return ';'.join(res)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities['one_name_per_set_str'] = entities.one_name_per_set.apply(list_to_str)\n",
    "entities['string_names_sets_str'] = entities.string_names_sets.apply(lists_to_str)\n",
    "# entities['stem_wratio_str'] = entities.stem_wratio.apply(ratio_to_str)              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get names entities that go with keywords and filter them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities['kw_names_tuples'] = entities.names_and_kw_str.apply(get_kw_name_tuples)\n",
    "entities['filt_kw_names_tuples'] = entities['kw_names_tuples'].apply(filter_by_score, iteration=1)\n",
    "\n",
    "entities['filt_kw_names_tuples'] = entities['filt_kw_names_tuples'].apply(filter_by_score, iteration=2)\n",
    "entities['filt_kw_names_list'] = entities['filt_kw_names_tuples'].apply(tuples_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities['filt_kw_names_str'] = entities.filt_kw_names_list.str.join('§')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities[['link', 'language', 'all_ent_str', 'name_and_kw', 'kw_and_ent',\n",
    "       'names_and_kw_str', 'names_str', 'names_sets_str', \n",
    "          'one_name_per_set_str', 'string_names_sets_str', 'filt_kw_names_str']].to_csv(filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
