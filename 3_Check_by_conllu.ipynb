{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Check Entities by dependecy trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import stanza\n",
    "import re\n",
    "from spacy_conll import init_parser\n",
    "from tokenization import *\n",
    "from keywords import keywords_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['link', 'language', 'all_ent_str', 'name_and_kw', 'kw_and_ent',\n",
       "       'names_and_kw_str', 'names_str', 'names_sets_str',\n",
       "       'one_name_per_set_str', 'string_names_sets_str', 'filt_kw_names_str'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = pd.read_csv('../data/entities_may.csv', index_col=[0])\n",
    "# entities = entities.set_index('link', drop=False)\n",
    "entities.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names_and_kw_str_to_list(names_and_kw_str):\n",
    "    if pd.notna(names_and_kw_str):\n",
    "        res = []\n",
    "        for ent in names_and_kw_str.split('<+>'):\n",
    "            parts = ent.split('<#>')\n",
    "            names = re.findall('(\\(\\d+\\, \\d+\\))<§§>PERS<§§>(.*?)<§§>([\\d\\.]+)', parts[2])\n",
    "            for n in names:\n",
    "                r = [int(i) for i in n[0].strip('()').split(', ')]\n",
    "                res.append( ( int(parts[0]), parts[1], r, n[1], float(n[2]) ) )\n",
    "        return res\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'text', 'subtitle', 'link', 'domain', 'datetime', 'views',\n",
       "       'created_at', 'category', 'language', 'domain_alias', 'mycategory',\n",
       "       'filt_kw_names_str', 'kw_and_ent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = pd.read_csv('../data/may.csv', index_col=[0])\n",
    "# news = news.set_index('link', drop=False)\n",
    "news['filt_kw_names_str'] = entities['filt_kw_names_str']\n",
    "news['kw_and_ent'] = entities.kw_and_ent\n",
    "news.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "news['filt_kw_names'] = news.filt_kw_names_str.str.split('§')\n",
    "news['entities'] = news.kw_and_ent.apply(names_and_kw_str_to_list)\n",
    "# news['sentenized'] = news.sentences_joined.str.split('<§>')\n",
    "\n",
    "news['checked_with_conllu'] = None\n",
    "\n",
    "news['all_text'] = news.title.str.cat(news.text, sep='\\n', na_rep = '')\n",
    "news['all_text'] = news.all_text.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 8s, sys: 5.03 s, total: 4min 13s\n",
      "Wall time: 4min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "news['sentenized'] = news.apply(lambda row: tokenize_to_sent_str(row.all_text, row.language), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading stanza models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 15.1MB/s]                    \n",
      "2021-07-10 18:04:00 INFO: Downloading default packages for language: uk (Ukrainian)...\n",
      "2021-07-10 18:04:02 INFO: File exists: /Users/oksana/stanza_resources/uk/default.zip.\n",
      "2021-07-10 18:04:06 INFO: Finished downloading models and saved to /Users/oksana/stanza_resources.\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 11.6MB/s]                    \n",
      "2021-07-10 18:04:06 INFO: Downloading these customized packages for language: ru (Russian)...\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "| pretrain  | gsd     |\n",
      "=======================\n",
      "\n",
      "2021-07-10 18:04:06 INFO: File exists: /Users/oksana/stanza_resources/ru/tokenize/gsd.pt.\n",
      "2021-07-10 18:04:06 INFO: File exists: /Users/oksana/stanza_resources/ru/pos/gsd.pt.\n",
      "2021-07-10 18:04:06 INFO: File exists: /Users/oksana/stanza_resources/ru/lemma/gsd.pt.\n",
      "2021-07-10 18:04:07 INFO: File exists: /Users/oksana/stanza_resources/ru/depparse/gsd.pt.\n",
      "2021-07-10 18:04:08 INFO: File exists: /Users/oksana/stanza_resources/ru/pretrain/gsd.pt.\n",
      "2021-07-10 18:04:08 INFO: Finished downloading models and saved to /Users/oksana/stanza_resources.\n",
      "2021-07-10 18:04:08 INFO: Loading these models for language: uk (Ukrainian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | iu      |\n",
      "| lemma     | iu      |\n",
      "=======================\n",
      "\n",
      "2021-07-10 18:04:08 INFO: Use device: cpu\n",
      "2021-07-10 18:04:08 INFO: Loading: tokenize\n",
      "2021-07-10 18:04:08 INFO: Loading: lemma\n",
      "2021-07-10 18:04:08 INFO: Done loading processors!\n",
      "2021-07-10 18:04:08 INFO: Loading these models for language: uk (Ukrainian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | iu      |\n",
      "| pos       | iu      |\n",
      "| lemma     | iu      |\n",
      "| depparse  | iu      |\n",
      "=======================\n",
      "\n",
      "2021-07-10 18:04:08 INFO: Use device: cpu\n",
      "2021-07-10 18:04:08 INFO: Loading: tokenize\n",
      "2021-07-10 18:04:08 INFO: Loading: pos\n",
      "2021-07-10 18:04:09 INFO: Loading: lemma\n",
      "2021-07-10 18:04:09 INFO: Loading: depparse\n",
      "2021-07-10 18:04:11 INFO: Done loading processors!\n",
      "2021-07-10 18:04:12 INFO: Loading these models for language: ru (Russian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "=======================\n",
      "\n",
      "2021-07-10 18:04:12 INFO: Use device: cpu\n",
      "2021-07-10 18:04:12 INFO: Loading: tokenize\n",
      "2021-07-10 18:04:12 INFO: Loading: pos\n",
      "2021-07-10 18:04:13 INFO: Loading: lemma\n",
      "2021-07-10 18:04:13 INFO: Loading: depparse\n",
      "2021-07-10 18:04:15 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza.download('uk')\n",
    "stanza.download('ru', package='gsd', processors='tokenize,pos,lemma,depparse')\n",
    "\n",
    "nlp = stanza.Pipeline('uk', processors='tokenize,lemma')\n",
    "\n",
    "nlp_uk = init_parser(\n",
    "        \"stanza\",\n",
    "        \"uk\", \n",
    "        is_tokenized = True,\n",
    "        include_headers=False,\n",
    "        parser_opts = {'processors': 'tokenize,pos,lemma,depparse'}\n",
    "    )\n",
    "\n",
    "nlp_ru = init_parser(\n",
    "        \"stanza\",\n",
    "        \"ru\", \n",
    "        is_tokenized = True,\n",
    "        include_headers=False,\n",
    "        parser_opts = {'package': 'gsd', 'processors': 'tokenize,pos,lemma,depparse'}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking sentences with keywords with dependency trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords = '|'.join([r'\\b' + kw for kw in keywords_dict])\n",
    "\n",
    "\n",
    "without_check = [\n",
    "    '(е|э)ксперт',\n",
    "    'анал(і|и)тик',\n",
    "    '(про)?цит(ує|ат|ував|увала?)',\n",
    "]\n",
    "according_to_keywords = [\n",
    "    'за\\s(словами|оцінк)',\n",
    "    'на\\sдумку',\n",
    "    'по\\sсловам',\n",
    "    'по\\s(оценк|мнен)',\n",
    "    'згідно',\n",
    "    'согласно',\n",
    "    'посилання',\n",
    "]\n",
    "wrong_keywords = [\n",
    "    'посила(вся|лася)',\n",
    "]\n",
    "\n",
    "without_check = '|'.join([r'\\b' + kw for kw in without_check])\n",
    "according_to_keywords = '|'.join([r'\\b' + kw for kw in according_to_keywords])\n",
    "wrong_keywords = '|'.join([r'\\b' + kw for kw in wrong_keywords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nsubj_relation(conllu_df, ent_range, all_keywords):\n",
    "    for i in ent_range:\n",
    "        id = i+1\n",
    "        curr_node = conllu_df.loc[id]\n",
    "        if curr_node['upostag'] in ['NOUN', 'PROPN']:\n",
    "            while curr_node['deprel'] != 'root':\n",
    "                head_node = conllu_df.loc[curr_node['head']]\n",
    "#                 print(curr_node.form, head_node.form)\n",
    "                if curr_node['deprel'].startswith('nsubj'):\n",
    "                    if re.match(all_keywords, head_node['form'], flags=re.I):\n",
    "                        return True\n",
    "                    else:\n",
    "                        return False\n",
    "                curr_node = head_node\n",
    "    return False\n",
    "\n",
    "\n",
    "def check_nmod_relation(conllu_df, ent_range, keyword):\n",
    "    for i in ent_range:\n",
    "        id = i+1\n",
    "        curr_node = conllu_df.loc[id]\n",
    "        if curr_node['upostag'] in ['NOUN', 'PROPN']:\n",
    "            while curr_node['deprel'] != 'root':\n",
    "                head_node = conllu_df.loc[curr_node['head']]\n",
    "                if re.match(keyword, curr_node['form'], flags=re.I) or \\\n",
    "                    re.match(keyword, head_node['form'], flags=re.I):\n",
    "                    return True\n",
    "                curr_node = head_node\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_check(entitites_list, sentences, filt_names_list, language):\n",
    "    if isinstance(entitites_list, list) and isinstance(filt_names_list, list):\n",
    "        res = []\n",
    "        sent_num = -1\n",
    "        for ent in entitites_list:\n",
    "            if (ent[3] in filt_names_list):\n",
    "                if re.search(wrong_keywords, ent[1], flags=re.I):\n",
    "                    res.append(False)\n",
    "                elif re.search(without_check, ent[1], flags=re.I):\n",
    "                    res.append(True)\n",
    "                else:\n",
    "                    if sent_num != ent[0]:\n",
    "                        sent_num = ent[0]\n",
    "                        if language == 'uk':\n",
    "                            doc = nlp_uk(sentences[sent_num])\n",
    "                        else:\n",
    "                            doc = nlp_ru(sentences[sent_num])\n",
    "                        conllu_df = (doc._.conll_pd).set_index('id')\n",
    "                    \n",
    "                    if re.search(according_to_keywords, ent[1], flags=re.I):\n",
    "                        keyword = ent[1].split()[-1]\n",
    "                        res.append(check_nmod_relation(conllu_df, ent[2][:-1], keyword))\n",
    "                    else:\n",
    "                        res.append(check_nsubj_relation(conllu_df, ent[2][:-1], all_keywords))        \n",
    "            else:\n",
    "                res.append('NINL')\n",
    "        try:\n",
    "            del conllu_df\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return res\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = 'checked_with_conllu_may.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news = news[['entities', 'sentenized', 'filt_kw_names', 'language', 'link', 'checked_with_conllu']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 62290\n",
      "100200 62408\n",
      "100400 62515\n",
      "100600 62629\n",
      "100800 62744\n",
      "101000 62878\n",
      "101200 63014\n",
      "101400 63137\n",
      "101600 63259\n",
      "101800 63373\n",
      "102000 63503\n",
      "102200 63636\n",
      "102400 63761\n",
      "102600 63900\n",
      "102800 64034\n",
      "103000 64175\n",
      "103200 64290\n",
      "103400 64404\n",
      "103600 64547\n",
      "103800 64690\n",
      "104000 64814\n",
      "104200 64945\n",
      "104400 65088\n",
      "104600 65176\n",
      "104800 65282\n",
      "105000 65392\n",
      "105200 65486\n",
      "105400 65596\n",
      "105600 65695\n",
      "105800 65811\n",
      "106000 65922\n",
      "106200 66028\n",
      "106400 66133\n",
      "106600 66255\n",
      "106800 66371\n",
      "107000 66477\n",
      "107200 66567\n",
      "107400 66675\n",
      "107600 66778\n",
      "107800 66863\n",
      "108000 66950\n",
      "108200 67047\n",
      "108400 67136\n",
      "108600 67235\n",
      "108800 67322\n",
      "109000 67416\n",
      "109200 67514\n",
      "109400 67605\n",
      "109600 67711\n",
      "109800 67812\n",
      "110000 67904\n",
      "110200 67991\n",
      "110400 68073\n",
      "110600 68170\n",
      "110800 68257\n",
      "111000 68364\n",
      "111200 68447\n",
      "111400 68519\n",
      "111600 68598\n",
      "111800 68689\n",
      "112000 68783\n",
      "112200 68871\n",
      "112400 68949\n",
      "112600 69035\n",
      "112800 69118\n",
      "113000 69246\n",
      "113200 69384\n",
      "113400 69510\n",
      "113600 69652\n",
      "113800 69771\n",
      "114000 69896\n",
      "114200 70031\n",
      "114400 70180\n",
      "114600 70304\n",
      "114800 70453\n",
      "115000 70606\n",
      "115200 70724\n",
      "115400 70838\n",
      "115600 70934\n",
      "115800 71043\n",
      "116000 71159\n",
      "116200 71270\n",
      "116400 71379\n",
      "116600 71479\n",
      "116800 71589\n",
      "117000 71699\n",
      "117200 71808\n",
      "117400 71912\n",
      "117600 72023\n",
      "117800 72133\n",
      "118000 72251\n",
      "118200 72378\n",
      "118400 72512\n",
      "118600 72624\n",
      "118800 72759\n",
      "119000 72894\n",
      "119200 73016\n",
      "119400 73124\n",
      "119600 73245\n",
      "119800 73358\n",
      "120000 73471\n",
      "120200 73521\n",
      "120400 73593\n",
      "120600 73664\n",
      "120800 73761\n",
      "121000 73839\n",
      "121200 73905\n",
      "121400 73977\n",
      "121600 74062\n",
      "121800 74140\n",
      "122000 74211\n",
      "122200 74266\n",
      "122400 74332\n",
      "122600 74393\n",
      "122800 74480\n",
      "123000 74567\n",
      "123200 74657\n",
      "123400 74726\n",
      "123600 74790\n",
      "123800 74868\n",
      "124000 74939\n",
      "124200 75020\n",
      "124400 75098\n",
      "124600 75190\n",
      "124800 75271\n",
      "125000 75356\n",
      "125200 75442\n",
      "125400 75535\n",
      "125600 75608\n",
      "125800 75694\n",
      "126000 75771\n",
      "126200 75845\n",
      "126400 75918\n",
      "126600 75989\n",
      "126800 76075\n",
      "127000 76134\n",
      "127200 76208\n",
      "127400 76288\n",
      "127600 76353\n",
      "127800 76428\n",
      "128000 76507\n",
      "128200 76596\n",
      "128400 76678\n",
      "128600 76740\n",
      "128800 76819\n",
      "129000 76923\n",
      "129200 77038\n",
      "129400 77155\n",
      "129600 77284\n",
      "129800 77415\n",
      "130000 77527\n",
      "130200 77654\n",
      "130400 77763\n",
      "130600 77884\n",
      "130800 78009\n",
      "131000 78129\n",
      "131200 78251\n",
      "131400 78379\n",
      "131600 78494\n",
      "131800 78614\n",
      "132000 78748\n",
      "132200 78858\n",
      "132400 78989\n",
      "132600 79104\n",
      "132800 79221\n",
      "133000 79336\n",
      "133200 79459\n",
      "133400 79589\n",
      "133600 79708\n",
      "133800 79824\n",
      "134000 79947\n",
      "134200 80088\n",
      "134400 80225\n",
      "134600 80360\n",
      "134800 80495\n",
      "135000 80610\n",
      "135200 80741\n",
      "135400 80872\n",
      "135600 80979\n",
      "135800 81069\n",
      "136000 81163\n",
      "136200 81261\n",
      "136400 81372\n",
      "136600 81477\n",
      "136800 81586\n",
      "137000 81688\n",
      "137200 81791\n",
      "137400 81899\n",
      "137600 82019\n",
      "137800 82135\n",
      "138000 82247\n",
      "138200 82346\n",
      "138400 82465\n",
      "138600 82568\n",
      "138800 82667\n",
      "139000 82787\n",
      "139200 82882\n",
      "139400 82980\n",
      "139600 83096\n",
      "139800 83200\n",
      "140000 83297\n",
      "140200 83403\n",
      "140400 83496\n",
      "140600 83592\n",
      "140800 83686\n",
      "141000 83785\n",
      "141200 83895\n",
      "141400 83999\n",
      "141600 84105\n",
      "141800 84204\n",
      "142000 84304\n",
      "142200 84406\n",
      "142400 84518\n",
      "142600 84627\n",
      "142800 84742\n",
      "143000 84828\n",
      "143200 84940\n",
      "143400 85044\n",
      "143600 85166\n",
      "143800 85279\n",
      "144000 85381\n",
      "144200 85490\n",
      "144400 85598\n",
      "144600 85701\n",
      "144800 85805\n",
      "145000 85899\n",
      "145200 85987\n",
      "145400 86082\n",
      "145600 86171\n",
      "145800 86275\n",
      "146000 86370\n",
      "146200 86482\n",
      "146400 86603\n",
      "146600 86714\n",
      "146800 86823\n",
      "147000 86936\n",
      "147200 87053\n",
      "147400 87165\n",
      "147600 87299\n",
      "147800 87441\n",
      "148000 87586\n",
      "148200 87713\n",
      "148400 87854\n",
      "148600 87986\n",
      "148800 88123\n",
      "149000 88257\n",
      "149200 88400\n",
      "149400 88549\n",
      "149600 88701\n",
      "149800 88822\n",
      "150000 88981\n",
      "150200 89125\n",
      "150400 89267\n",
      "150600 89413\n",
      "150800 89552\n",
      "151000 89674\n",
      "151200 89782\n",
      "CPU times: user 3h 3min 17s, sys: 10min 9s, total: 3h 13min 27s\n",
      "Wall time: 3h 11min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start = 0\n",
    "end = len(news)\n",
    "step = 200\n",
    "\n",
    "for k in range(start, end, step):\n",
    "    part = news.iloc[k:k+step].apply(lambda row: sentence_check(row.entities, row.sentenized, row.filt_kw_names, row.language), axis=1)\n",
    "\n",
    "    news['checked_with_conllu'].update(part)\n",
    "    news['checked_with_conllu'].to_csv(out_file)\n",
    "    print(k, news['checked_with_conllu'].notna().sum())\n",
    "    \n",
    "    del part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "news[['link', 'checked_with_conllu']].to_csv('conllu_checked_may.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = \"Головна економістка інвестиційної компанії Dragon CapitaІ Олена Білан сказала виданню, що у грудні уряду необхідно буде залучити приблизно $3 млрд, щоб профінансувати дефіцит бюджету, не урізавши критичних витрат.\"\n",
    "# doc = nlp_uk(s)\n",
    "# conll = doc._.conll_pd\n",
    "# print(conll)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
