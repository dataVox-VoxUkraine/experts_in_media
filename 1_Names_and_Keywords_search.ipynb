{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Searching for name entities and keywords in news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mitie import *\n",
    "from collections import Counter, OrderedDict\n",
    "import re\n",
    "from tokenization import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_uk_model = '../NER_models/uk_model.dat'\n",
    "path_to_ru_model = '../NER_models/ru_model.dat'\n",
    "news_filepath = '../data/may.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_uk = named_entity_extractor(path_to_uk_model)\n",
    "ner_ru = named_entity_extractor(path_to_ru_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'title', 'text', 'subtitle', 'link', 'domain', 'datetime',\n",
       "       'views', 'created_at', 'category', 'language', 'domain_alias',\n",
       "       'mycategory'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = pd.read_csv(news_filepath, nrows=1000)\n",
    "news.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_uk_path = '../dicts/names_ukr.txt' \n",
    "names_ru_path = '../dicts/names_ru.txt' \n",
    "with open(names_uk_path) as f:\n",
    "    names_uk = [name.strip() for name in f.readlines()]\n",
    "    \n",
    "with open(names_ru_path) as f:\n",
    "    names_ru = [name.strip() for name in f.readlines()]\n",
    "names = list(set(names_uk+names_ru))\n",
    "names_string = '|'.join(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "news['all_text'] = news.title.str.cat(news.text, sep='\\n', na_rep = '')\n",
    "news['all_text'] = news.all_text.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding all entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all entities in every sentence of tokenized text\n",
    "def find_entities(tokens, ner_model):\n",
    "    entities_in_sentences = []\n",
    "    for sentence in tokens:\n",
    "        entities = ner_model.extract_entities(sentence)\n",
    "        entities_in_sentences.append(entities)\n",
    "    return entities_in_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run find_entities for all df \n",
    "def process_news(news, start=0, finish=1000, step=100):\n",
    "    for k in range(start, finish, step):\n",
    "        try:\n",
    "            del news_part\n",
    "        except:\n",
    "            pass    \n",
    "        news_part = news.iloc[k:k + step].copy()\n",
    "        news_part['entities'] = news_part.apply(lambda row: \\\n",
    "                                                find_entities(row.tokenized, ner_uk) if row.language=='uk' \\\n",
    "                                                else find_entities(row.tokenized, ner_ru), axis=1)\n",
    "        news.entities.update(news_part.entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 786 ms, sys: 23.6 ms, total: 809 ms\n",
      "Wall time: 1.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "news['tokenized'] = news.apply(lambda row: tokenize(row.all_text, row.language), axis=1)\n",
    "news['entities'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.04 s, sys: 196 ms, total: 5.24 s\n",
      "Wall time: 5.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "process_news(news, start=0, finish=len(news), step=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for joining list into strings and splitting strings into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_kw_and_names(kw_and_ent):\n",
    "    if pd.notna(kw_and_ent) and (kw_and_ent!=''):\n",
    "        n = kw_and_ent.count('PERS')\n",
    "        res = []\n",
    "        for sent in kw_and_ent.split('<+>'):\n",
    "                sent_num, kw, entities = sent.split('<#>', maxsplit=2)\n",
    "                names_scores = re.findall(r'PERS<§§>(.*?)<§§>([\\d\\.]+)', entities)\n",
    "                for ns in names_scores:\n",
    "                    res.append((sent_num, kw, ns[0], ns[1]))\n",
    "        if n!=len(res):\n",
    "            print(kw_and_ent)\n",
    "        return res\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names_and_kw_to_string(names_and_kw):\n",
    "    if isinstance(names_and_kw, list):\n",
    "        res = []\n",
    "        for name in names_and_kw:\n",
    "            res.append('<+>'.join(name))\n",
    "        return '@+@'.join(res)\n",
    "    return None\n",
    "\n",
    "def names_and_kw_to_list(names_and_kw_str):\n",
    "    if pd.notnull(names_and_kw_str):\n",
    "        res = []\n",
    "        for ent in names_and_kw_str.split('@+@'):\n",
    "            res.append(ent.split('<+>'))\n",
    "        return res\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save entitites list to string to write to csv file\n",
    "def entities_to_string(entities, tokens):\n",
    "    res = []\n",
    "    for s in range(len(entities)):\n",
    "        sent_res= []\n",
    "        for ent in entities[s]:\n",
    "            entity_text = \" \".join(tokens[s][i] for i in ent[0])\n",
    "            ent_range=str(ent[0]).strip('range')\n",
    "            sent_res.append('<§§>'.join([ent_range, ent[1], entity_text, str(round(ent[2], 2))]))\n",
    "        res.append('<;;>'.join(sent_res))\n",
    "    return '<@@>'.join(res)\n",
    "\n",
    "\n",
    "#parse entitites strings into lists \n",
    "def ent_string_to_list(entity):\n",
    "    res = []\n",
    "    for sent in entity.split('++'):\n",
    "        sent_entities=[]\n",
    "        for ent in sent.split(';;'):\n",
    "            ent_parts = ent.split('::')\n",
    "            sent_entities.append(ent_parts)\n",
    "        res.append(sent_entities)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only sentences that have PERS entity and a keyword\n",
    "# save sentence number, keywords and entities to string\n",
    "def keywords_and_names(name_and_kw, entities):\n",
    "    res = []\n",
    "    sentences = name_and_kw.split('#')\n",
    "    for i in range(len(sentences)):\n",
    "        if sentences[i] not in ['NPR','NKW']:\n",
    "            res.append('<#>'.join([str(i), sentences[i], entities[i]]))\n",
    "    return '<+>'.join(res)\n",
    "\n",
    "\n",
    "# find keywords in sentences with PERS entity\n",
    "# save result to string\n",
    "def find_keywords(sentences, entities):\n",
    "    res=[]\n",
    "    if isinstance(entities, list):\n",
    "        for s in range(len(sentences)):\n",
    "            if 'PERS' in entities[s]:\n",
    "                kw = re.search(keywords, sentences[s], flags=re.I)\n",
    "                if kw:\n",
    "                    res.append(kw.group())\n",
    "                else:\n",
    "                    res.append('NKW')\n",
    "            else:\n",
    "                res.append('NPR')\n",
    "    else:\n",
    "        res.append('NPR')\n",
    "    return '#'.join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "news['all_ent_str'] = news.apply(lambda row: entities_to_string(row.entities, row.tokenized), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "news['sentenized'] = news.apply(lambda row: text_to_sent(row.all_text, row.language), axis=1)\n",
    "\n",
    "news['ent_sent'] = news.all_ent_str.str.split('<@@>')\n",
    "\n",
    "news['name_and_kw'] = news.apply(lambda row: find_keywords(row.sentenized, row.ent_sent), axis=1)\n",
    "\n",
    "news['kw_and_ent'] = news.apply(lambda row: keywords_and_names(row.name_and_kw, row.ent_sent), axis=1)\n",
    "\n",
    "news['names_and_kw'] = news.kw_and_ent.apply(get_only_kw_and_names)\n",
    "\n",
    "news['names_and_kw_str'] = news.names_and_kw.apply(names_and_kw_to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'text', 'subtitle', 'link', 'domain', 'datetime', 'views',\n",
       "       'created_at', 'category', 'language', 'domain_alias', 'all_text',\n",
       "       'tokenized', 'entities', 'all_ent_str', 'sentenized', 'ent_sent',\n",
       "       'name_and_kw', 'kw_and_ent', 'names_and_kw', 'names_and_kw_str',\n",
       "       'mycategory'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "news['kw_and_ent'] = news.kw_and_ent.apply(lambda x: None if x=='' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "news['names_str'] = news.names_and_kw.apply(lambda ents:\n",
    "                                                '§'.join([e[2] for e in ents]) if isinstance(ents, list) else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_dict = ['(е|э)ксперт',\n",
    "'анал(і|и)тик',\n",
    "'(про)?анал(і|и)з',\n",
    "'розпові(в|ла|дає|дав|дала)',\n",
    "'(расс|роз|выс?|вис?|до|в|у|с)?каз(ав|ала?|ати|увати|ував|увала|ывала?|ывать|ує|ывает)',\n",
    "'відсто(ює|яв|яла|ював|ювала)',\n",
    "'(за|объ)яв(ила?|ив|ляє|ляет|ляв|ляла?)',\n",
    "'пишет?',\n",
    "'(на)?писа(в|ла?)',\n",
    "'повідом(ила|ив|ляє|ляв|ляла)',\n",
    "'посила(нням?|ється|вся|лася)',\n",
    "'(про)?комм?ент(ував|ує|увала|ирует|ировала?|ар)',\n",
    "'констат(ує|ував|увала|ирует|ировала?)',\n",
    "'зауваж(ив|ила|ує|ував|увала)',\n",
    "'зверну(в|ла)(с(я|ь))',\n",
    "'звер(тав|таєть|тала)(с(я|ь))',\n",
    "'звер(нув|нула|тає|тала|тав)\\sувагу',\n",
    "'акцент(ує|ував|увала|ирует|ировала?)',\n",
    "'вислов(ив|ила|ити|лювала|лював)(с(я|ь))?',\n",
    "'в(и|ы)с(унув|ував|унула?|увала)',\n",
    "'вважа(є|в|ла)',\n",
    "'говор(ить?|ив|ила?)',\n",
    "'застеріг(ає|ав|ала)?',\n",
    "'застерегла'\n",
    "'запевн(яє|яла|яв|ив|ила)',\n",
    "'(у|в)певнен(ий|а)',\n",
    "'(с|під)тверд(жує|жував|жувала|ив|ила)',\n",
    "'(у|под)твержд(ает|ала?)',\n",
    "'(у|под)твердила?',\n",
    "'(за|від)знач(ив|ила|ає|ав|ала)',\n",
    "'каже',\n",
    "'(по)?рад(ить|ив|ила)',\n",
    "'(до|при)трим(ав|ала|уєть|ував|увала)с(я|ь)\\sдумк',\n",
    "'дум(ає|ав|ает|ала?)',\n",
    "'за\\s(словами|оцінк)',\n",
    "'згідно\\sз',\n",
    "'согласно'\n",
    "'відповідно\\sдо',\n",
    "'до(да|бав)(в|ила?|ла|є|ляет)',\n",
    "'(на|о)голо(шує|сив|сила|шував|шувала|шувати)',\n",
    "'наз(ы|и)?ва(ет|є|в|ла?)',\n",
    "'з?роби(в|ла|ть)\\s(виснов|заяв)',\n",
    "'перекон(ує|ав|ував|ала|увала|аний|ана)',\n",
    "'на\\sдумку',\n",
    "'(про)?цит(ує|ат|ував|увала?)',\n",
    "'озвуч(ив|ла?)',\n",
    "'підкресл(ює|ив|ила|ював|ювала)',\n",
    "'(від|під)мі(чає|тив|тила|чав|чала)',\n",
    "'спостер(ігав|ігала|еження|ігає)',\n",
    "'визна(є|в|ла|вав|вала|ти|вати)',\n",
    "'с?прогноз(ує|ував|увала|ировала?|ирует)',\n",
    "'п(і|о)дозр(ює|евает|ював|ювала|евала?)',\n",
    "'(при|до)пу(скає|скав|стив|скала|стила|щення|стити)',\n",
    "'(за)?пропон(ував|увала)',\n",
    "'дов(ів|ела|одить|одила|одив)',\n",
    "'дон(іс|есла|осить?|осив|осила|ести|ес)',\n",
    "'(по)?рекоменд(ує|ував|ац|увала|ует|овала?)',\n",
    "'аргумент(ує|ував|увала|ирует|ировала?)?',\n",
    "'повтор(ив|ила?|ює|ювала|ював)',\n",
    "'дізна(ли|в|ла|вав|вали|вала)(с(я|ь))',\n",
    "'(за|роз|с)пит(али|итав|ала)',\n",
    "'уточн(ив|ила?|ював|ення|ювала)',\n",
    "'призы?в(ала?|ает)',\n",
    "'заклика(є|в|ла)',\n",
    "'(от|под|за)ме(тила?|чает|чала?)',\n",
    "'по\\sмнению',\n",
    "'по\\sсловам',\n",
    "'п(о|і)дтвер(ждает|джує|див|джував|джувала|дила?|ждала?)',\n",
    "'сообщ(ать|ила?|ение)',\n",
    "'р(о|а)збир(ала?|ав|али)(с(я|ь))?',\n",
    "'р(о|а)з(і|о)бр(ала?|ав|али)(с(я|ь))?',\n",
    "'(рас)?спросил',\n",
    "'узна(вал|л)',\n",
    "'по\\s(оценк|мнен)',\n",
    "'счита(ет|ю|ла?)',\n",
    "'рассчитыва(ет|ю|ла?)',\n",
    "'рассчитала?',\n",
    "'розрахову(є|вав|вала)',\n",
    "'(роз|по)раху(вав|вала)',                 \n",
    "'(рас|о)цен(ивает|ивала?|ила?)',\n",
    "'(роз|о)цін(ює|ював|ювала|ив|ила)',\n",
    "'(по)?совет(ует|овала?)',\n",
    "'подчерк(нула?|ивает)',\n",
    "'ссыл(кой|ается|ался|алась)',\n",
    "'обра(ти|ща)(л.?с)',\n",
    "'обра(ти|ща)(ла?|ет)\\sвниман',\n",
    "'предостере(г|гла|гать|гала?|жен)',\n",
    "'(у|за)вер(ена?|ять|яла?|ила?)',\n",
    "'(при|до)держ(ивать|ала?|ивала?)с(я|ь)',\n",
    "'(по|объ)ясн(ила?|яла?|ить|ять|ение)',\n",
    "'с?дел(ать|ала?|ает)\\s(вывод|заяв)',\n",
    "'при(шел|шла|ходит)\\sк\\sвывод',\n",
    "'заключ(ила?|ает)',\n",
    "'убеж?д(ена?|ает|ала?|ила?)',\n",
    "'наблюд(ает|ала?)',\n",
    "'призна(ет|ла?|вала?)',\n",
    "'пред(по)?л(аг|ож)(ает|ала?|ила?|ени)',\n",
    "'(от|на)стаив(ает|ала?)',\n",
    "'повтор(яет|ила?|яла?)',\n",
    "'в(ы|и)ра(ж|з)(ила?|ает|ає|ив|ав|ала?)',\n",
    "'рассу(ждает|ждала?|дила?)',\n",
    "'(роз)?мірк(овує|овував|овувала)',\n",
    "'підсумува(в|ла)',\n",
    "'подытожила?',\n",
    "'п?о?зна(к.м|йо)(ила?|мив|мила|млював|млювала|ливала?)'\n",
    "]\n",
    "keywords = '|'.join([r'\\b' + kw for kw in keywords_dict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'text', 'subtitle', 'link', 'domain', 'datetime', 'views',\n",
       "       'created_at', 'category', 'language', 'domain_alias', 'all_text',\n",
       "       'tokenized', 'entities', 'all_ent_str', 'sentenized', 'ent_sent',\n",
       "       'name_and_kw', 'kw_and_ent', 'names_and_kw', 'names_and_kw_str',\n",
       "       'mycategory', 'names_str', 'ent_strings'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "news[['title', 'text', 'subtitle', 'link', 'domain', 'datetime', 'views',\n",
    "       'created_at', 'category', 'language', 'domain_alias', \n",
    "       'mycategory']].to_csv(news_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "news['ent_strings'] = news.apply(lambda row: entities_to_string(row.entities, row.tokenized), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_path = '../data/entities_may.csv'\n",
    "news[['link', 'language', 'all_ent_str', \n",
    "       'name_and_kw', 'kw_and_ent', 'names_and_kw_str',\n",
    "       'names_str']].to_csv(entities_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news.drop(columns=['all_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'text', 'subtitle', 'link', 'domain', 'datetime', 'views',\n",
       "       'created_at', 'category', 'language', 'id', 'domain_alias', 'all_text',\n",
       "       'tokenized', 'entities', 'all_ent_str', 'sentenized', 'ent_sent',\n",
       "       'name_and_kw', 'kw_and_ent', 'names_and_kw', 'names_and_kw_str',\n",
       "       'names_str'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
